"""
Boston Housing - Yapay Sinir AÄŸlarÄ± ile Ev FiyatÄ± Tahmini
=========================================================
Bu proje, ev Ã¶zelliklerine gÃ¶re fiyat tahmini yapmak iÃ§in 
yapay sinir aÄŸlarÄ± (Multi-Layer Perceptron) kullanÄ±r.
"""

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.neural_network import MLPRegressor
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')
import pickle

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("=" * 70)
print("BOSTON HOUSING - YAPAY SÄ°NÄ°R AÄLARI Ä°LE EV FÄ°YATI TAHMÄ°NÄ°")
print("=" * 70)

# 1. VERÄ° YÃœKLEME
print("\nğŸ“ Veri yÃ¼kleniyor...")
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 
                'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

df = pd.read_csv('housing.csv', delim_whitespace=True, header=None, names=column_names)

print(f"\nâœ“ Veri baÅŸarÄ±yla yÃ¼klendi!")
print(f"  Toplam Ã¶rnek sayÄ±sÄ±: {len(df)}")
print(f"  Ã–zellik sayÄ±sÄ±: {len(df.columns) - 1}")

# 2. VERÄ° KEÅFÄ°
print("\n" + "=" * 70)
print("ğŸ“Š VERÄ° SETÄ° HAKKINDA BÄ°LGÄ°LER")
print("=" * 70)

print("\nğŸ” Ä°lk 5 kayÄ±t:")
print(df.head())

print("\nğŸ“ˆ Ä°statistiksel Ã–zet:")
print(df.describe().round(2))

print("\nğŸ·ï¸ Ã–zellik AÃ§Ä±klamalarÄ±:")
feature_descriptions = {
    'CRIM': 'SuÃ§ oranÄ± (per capita crime rate)',
    'ZN': 'Konut alanÄ± oranÄ± (>25,000 sq.ft)',
    'INDUS': 'Ticari alan oranÄ±',
    'CHAS': 'Charles River yakÄ±nlÄ±ÄŸÄ± (0/1)',
    'NOX': 'Azot oksit konsantrasyonu',
    'RM': 'Ortalama oda sayÄ±sÄ±',
    'AGE': 'Eski ev oranÄ± (1940 Ã¶ncesi)',
    'DIS': 'Ä°stihdam merkezlerine uzaklÄ±k',
    'RAD': 'Otoyol eriÅŸim indeksi',
    'TAX': 'Emlak vergisi oranÄ±',
    'PTRATIO': 'Ã–ÄŸrenci-Ã¶ÄŸretmen oranÄ±',
    'B': 'Siyahi nÃ¼fus oranÄ±',
    'LSTAT': 'DÃ¼ÅŸÃ¼k statÃ¼lÃ¼ nÃ¼fus yÃ¼zdesi',
    'MEDV': 'ğŸ¯ Hedef: Ev fiyatÄ± (bin $)'
}

for feature, description in feature_descriptions.items():
    print(f"  â€¢ {feature:8} - {description}")

# Eksik veri kontrolÃ¼
print(f"\nâŒ Eksik deÄŸer: {df.isnull().sum().sum()}")

# 3. VERÄ° GÃ–RSELLEÅTÄ°RME
print("\n" + "=" * 70)
print("ğŸ“Š VERÄ° GÃ–RSELLEÅTÄ°RME")
print("=" * 70)

# Hedef deÄŸiÅŸkenin daÄŸÄ±lÄ±mÄ±
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Fiyat daÄŸÄ±lÄ±mÄ±
axes[0, 0].hist(df['MEDV'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
axes[0, 0].set_xlabel('Ev FiyatÄ± ($1000)', fontsize=11)
axes[0, 0].set_ylabel('Frekans', fontsize=11)
axes[0, 0].set_title('Ev FiyatlarÄ±nÄ±n DaÄŸÄ±lÄ±mÄ±', fontsize=13, fontweight='bold')
axes[0, 0].axvline(df['MEDV'].mean(), color='red', linestyle='--', linewidth=2, 
                   label=f'Ortalama: ${df["MEDV"].mean():.1f}k')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# En Ã¶nemli Ã¶zelliklerle iliÅŸki
important_features = ['RM', 'LSTAT', 'PTRATIO']
for idx, feature in enumerate(important_features):
    row = (idx + 1) // 2
    col = (idx + 1) % 2
    axes[row, col].scatter(df[feature], df['MEDV'], alpha=0.5, s=30, color='steelblue')
    axes[row, col].set_xlabel(feature_descriptions[feature], fontsize=11)
    axes[row, col].set_ylabel('Ev FiyatÄ± ($1000)', fontsize=11)
    axes[row, col].set_title(f'{feature} vs Ev FiyatÄ±', fontsize=12, fontweight='bold')
    axes[row, col].grid(True, alpha=0.3)
    
    # Trend Ã§izgisi
    z = np.polyfit(df[feature], df['MEDV'], 1)
    p = np.poly1d(z)
    axes[row, col].plot(df[feature], p(df[feature]), "r--", alpha=0.8, linewidth=2, label='Trend')
    axes[row, col].legend()

plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/1_veri_gorsellestirme.png', dpi=300, bbox_inches='tight')
print("âœ“ GÃ¶rsel kaydedildi: 1_veri_gorsellestirme.png")
plt.close()

# Korelasyon matrisi
plt.figure(figsize=(14, 10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Ã–zellikler ArasÄ± Korelasyon Matrisi', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/2_korelasyon_matrisi.png', dpi=300, bbox_inches='tight')
print("âœ“ GÃ¶rsel kaydedildi: 2_korelasyon_matrisi.png")
plt.close()

# En yÃ¼ksek korelasyonlar
print("\nğŸ”— Ev FiyatÄ± ile En YÃ¼ksek Korelasyonlar:")
correlations = df.corr()['MEDV'].sort_values(ascending=False)
for feature, corr in correlations.items():
    if feature != 'MEDV':
        emoji = "ğŸ“ˆ" if corr > 0 else "ğŸ“‰"
        print(f"  {emoji} {feature:8} : {corr:+.3f}")

# 4. VERÄ° HAZIRLIÄI
print("\n" + "=" * 70)
print("ğŸ”§ VERÄ° HAZIRLANIYOR")
print("=" * 70)

# Ã–zellikler ve hedef deÄŸiÅŸken
X = df.drop('MEDV', axis=1)
y = df['MEDV']

print(f"\nâœ“ Ã–zellikler (X): {X.shape}")
print(f"âœ“ Hedef (y): {y.shape}")

# Veriyi eÄŸitim ve test setlerine ayÄ±rma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nğŸ“Š Veri Setleri:")
print(f"  â€¢ EÄŸitim seti: {X_train.shape[0]} Ã¶rnek")
print(f"  â€¢ Test seti: {X_test.shape[0]} Ã¶rnek")

# Veriyi Ã¶lÃ§eklendirme (Normalization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ“ Veri standardize edildi (StandardScaler)")
print(f"  Ortalama: {X_train_scaled.mean():.6f}")
print(f"  Standart sapma: {X_train_scaled.std():.6f}")

# 5. YAPAY SÄ°NÄ°R AÄI MODELÄ° OLUÅTURMA
print("\n" + "=" * 70)
print("ğŸ§  YAPAY SÄ°NÄ°R AÄI MODELÄ° OLUÅTURULUYOR")
print("=" * 70)

# Model mimarisi
# MLPRegressor = Multi-Layer Perceptron (Ã‡ok KatmanlÄ± AlgÄ±layÄ±cÄ±)
model = MLPRegressor(
    hidden_layer_sizes=(128, 64, 32, 16),  # 4 gizli katman
    activation='relu',                      # ReLU aktivasyon fonksiyonu
    solver='adam',                          # Adam optimizer
    alpha=0.001,                            # L2 regularization
    batch_size=32,                          # Mini-batch boyutu
    learning_rate='adaptive',               # Adaptif Ã¶ÄŸrenme oranÄ±
    learning_rate_init=0.001,               # BaÅŸlangÄ±Ã§ Ã¶ÄŸrenme oranÄ±
    max_iter=1000,                          # Maksimum epoch
    early_stopping=True,                    # Erken durdurma
    validation_fraction=0.2,                # Validation set oranÄ±
    n_iter_no_change=50,                    # Erken durdurma patience
    verbose=False,                          # Sessiz mod
    random_state=42
)

print("\nğŸ—ï¸ Model Mimarisi:")
print(f"  â€¢ GiriÅŸ katmanÄ±: {X_train_scaled.shape[1]} nÃ¶ron")
print(f"  â€¢ Gizli katman 1: 128 nÃ¶ron (ReLU)")
print(f"  â€¢ Gizli katman 2: 64 nÃ¶ron (ReLU)")
print(f"  â€¢ Gizli katman 3: 32 nÃ¶ron (ReLU)")
print(f"  â€¢ Gizli katman 4: 16 nÃ¶ron (ReLU)")
print(f"  â€¢ Ã‡Ä±kÄ±ÅŸ katmanÄ±: 1 nÃ¶ron (Linear)")

print("\nâš™ï¸ Model Parametreleri:")
print(f"  â€¢ Optimizer: Adam")
print(f"  â€¢ Ã–ÄŸrenme oranÄ±: 0.001 (adaptive)")
print(f"  â€¢ Batch size: 32")
print(f"  â€¢ Max epoch: 1000")
print(f"  â€¢ Early stopping: Aktif (patience=50)")
print(f"  â€¢ L2 regularization (alpha): 0.001")

# 6. MODEL EÄÄ°TÄ°MÄ°
print("\n" + "=" * 70)
print("ğŸ¯ MODEL EÄÄ°TÄ°LÄ°YOR")
print("=" * 70)
print("\nğŸš€ EÄŸitim baÅŸlÄ±yor...\n")

# EÄŸitim
model.fit(X_train_scaled, y_train)

print(f"\nâœ“ Model eÄŸitimi tamamlandÄ±!")
print(f"  â€¢ Toplam iterasyon: {model.n_iter_}")
print(f"  â€¢ Son loss deÄŸeri: {model.loss_:.6f}")

# 7. MODEL PERFORMANSI
print("\n" + "=" * 70)
print("ğŸ“ˆ MODEL PERFORMANSI")
print("=" * 70)

# Tahminler
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

# Metrikler
def calculate_metrics(y_true, y_pred, set_name):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    
    print(f"\n{set_name} Seti SonuÃ§larÄ±:")
    print(f"  â€¢ RÂ² Score (AÃ§Ä±klama GÃ¼cÃ¼): {r2:.4f} ({r2*100:.2f}%)")
    print(f"  â€¢ RMSE (KÃ¶k Ortalama Kare HatasÄ±): ${rmse:.2f}k")
    print(f"  â€¢ MAE (Ortalama Mutlak Hata): ${mae:.2f}k")
    print(f"  â€¢ MSE (Ortalama Kare HatasÄ±): {mse:.2f}")
    
    return mse, rmse, mae, r2

train_metrics = calculate_metrics(y_train, y_train_pred, "ğŸ“ EÄŸitim")
test_metrics = calculate_metrics(y_test, y_test_pred, "ğŸ§ª Test")

# Overfitting kontrolÃ¼
overfit_check = train_metrics[3] - test_metrics[3]
print(f"\nğŸ“Š Overfitting KontrolÃ¼:")
print(f"  â€¢ RÂ² farkÄ± (Train - Test): {overfit_check:.4f}")
if overfit_check < 0.05:
    print(f"  âœ“ Model iyi genelleÅŸtirilmiÅŸ (Overfitting YOK)")
elif overfit_check < 0.15:
    print(f"  âš ï¸ Hafif overfitting var")
else:
    print(f"  âŒ Ciddi overfitting var")

# Cross-validation
print("\nğŸ”„ Cross-Validation (5-Fold):")
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, 
                            scoring='r2', n_jobs=-1)
print(f"  â€¢ Ortalama RÂ²: {cv_scores.mean():.4f}")
print(f"  â€¢ Standart sapma: {cv_scores.std():.4f}")
print(f"  â€¢ TÃ¼m skorlar: {[f'{s:.3f}' for s in cv_scores]}")

# 8. GÃ–RSELLEÅTÄ°RME - EÄÄ°TÄ°M SÃœRECÄ°
print("\n" + "=" * 70)
print("ğŸ“Š SONUÃ‡LAR GÃ–RSELLEÅTÄ°RÄ°LÄ°YOR")
print("=" * 70)

# EÄŸitim geÃ§miÅŸi
fig, ax = plt.subplots(1, 1, figsize=(12, 6))

# Loss grafiÄŸi (loss_curve_ sadece early_stopping=True ise var)
if hasattr(model, 'loss_curve_'):
    ax.plot(model.loss_curve_, linewidth=2, color='steelblue', label='Training Loss')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss (MSE)', fontsize=12)
    ax.set_title('Model Loss DeÄŸiÅŸimi', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # Best loss noktasÄ±
    best_iter = np.argmin(model.loss_curve_)
    ax.axvline(best_iter, color='red', linestyle='--', alpha=0.5, 
               label=f'Best iteration: {best_iter}')
    ax.legend(fontsize=11)

plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/3_egitim_sureci.png', dpi=300, bbox_inches='tight')
print("âœ“ GÃ¶rsel kaydedildi: 3_egitim_sureci.png")
plt.close()

# 9. TAHMÄ°N vs GERÃ‡EK DEÄERLER
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Test seti tahminleri
axes[0].scatter(y_test, y_test_pred, alpha=0.6, s=50, color='steelblue', edgecolors='darkblue', linewidth=0.5)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
             'r--', lw=3, label='MÃ¼kemmel Tahmin')
axes[0].set_xlabel('GerÃ§ek Fiyat ($1000)', fontsize=12)
axes[0].set_ylabel('Tahmin Edilen Fiyat ($1000)', fontsize=12)
axes[0].set_title(f'Test Seti: GerÃ§ek vs Tahmin\nRÂ² = {test_metrics[3]:.3f}', 
                  fontsize=13, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Hata daÄŸÄ±lÄ±mÄ±
errors = y_test - y_test_pred
axes[1].hist(errors, bins=30, color='coral', edgecolor='darkred', alpha=0.7, linewidth=1)
axes[1].axvline(0, color='red', linestyle='--', linewidth=3, label='Hata=0')
axes[1].axvline(errors.mean(), color='blue', linestyle='--', linewidth=2, 
                label=f'Ortalama: ${errors.mean():.2f}k')
axes[1].set_xlabel('Tahmin HatasÄ± ($1000)', fontsize=12)
axes[1].set_ylabel('Frekans', fontsize=12)
axes[1].set_title(f'Hata DaÄŸÄ±lÄ±mÄ±\nStd: ${errors.std():.2f}k', 
                  fontsize=13, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/4_tahmin_sonuclari.png', dpi=300, bbox_inches='tight')
print("âœ“ GÃ¶rsel kaydedildi: 4_tahmin_sonuclari.png")
plt.close()

# 10. Ã–RNEK TAHMÄ°NLER
print("\n" + "=" * 70)
print("ğŸ  Ã–RNEK TAHMÄ°NLER")
print("=" * 70)

# Rastgele 15 Ã¶rnek seÃ§
sample_indices = np.random.choice(len(X_test), min(15, len(X_test)), replace=False)
samples = X_test.iloc[sample_indices]
samples_scaled = scaler.transform(samples)
predictions = model.predict(samples_scaled)
actuals = y_test.iloc[sample_indices].values

print("\n  #  | GerÃ§ek ($k) | Tahmin ($k) | Fark ($k) | Hata %  | Durum")
print("-" * 75)
for i, (actual, pred) in enumerate(zip(actuals, predictions), 1):
    diff = actual - pred
    error_pct = abs(diff) / actual * 100
    status = "âœ“" if error_pct < 15 else "âš " if error_pct < 25 else "âœ—"
    print(f"{i:3d} | {actual:11.2f} | {pred:11.2f} | {diff:9.2f} | {error_pct:6.2f}% | {status}")

avg_error = np.mean(np.abs(actuals - predictions))
print(f"\nOrtalama Mutlak Hata: ${avg_error:.2f}k")

# 11. MODEL KAYDETME
print("\n" + "=" * 70)
print("ğŸ’¾ MODEL KAYDEDÄ°LÄ°YOR")
print("=" * 70)

# Model kaydet
with open('/mnt/user-data/outputs/ev_fiyat_modeli.pkl', 'wb') as f:
    pickle.dump(model, f)
print("âœ“ Model kaydedildi: ev_fiyat_modeli.pkl")

# Scaler'Ä± kaydet
with open('/mnt/user-data/outputs/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("âœ“ Scaler kaydedildi: scaler.pkl")

# 12. Ã–ZELLÄ°K Ã–NEMLÄ°LÄ°ÄÄ° (Permutation Importance)
print("\n" + "=" * 70)
print("ğŸ” Ã–ZELLÄ°K Ã–NEMLÄ°LÄ°ÄÄ° ANALÄ°ZÄ°")
print("=" * 70)

result = permutation_importance(
    model, X_test_scaled, y_test, 
    n_repeats=10, random_state=42, n_jobs=-1
)

# SonuÃ§larÄ± sÄ±rala
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': result.importances_mean,
    'std': result.importances_std
}).sort_values('importance', ascending=False)

print("\nğŸ“Š Ã–zellik Ã–nemlilikleri:")
for idx, row in feature_importance.iterrows():
    bar = "â–ˆ" * int(row['importance'] * 50)
    print(f"  {row['feature']:8} : {row['importance']:6.4f} (Â±{row['std']:.4f}) {bar}")

# GÃ¶rselleÅŸtir
plt.figure(figsize=(12, 8))
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(feature_importance)))
plt.barh(feature_importance['feature'], feature_importance['importance'], 
         xerr=feature_importance['std'], color=colors, alpha=0.8, edgecolor='black', linewidth=1)
plt.xlabel('Ã–nemlilik (Permutation Importance)', fontsize=12)
plt.ylabel('Ã–zellikler', fontsize=12)
plt.title('Yapay Sinir AÄŸÄ± - Ã–zellik Ã–nemlilikleri', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('/mnt/user-data/outputs/5_ozellik_onemliligi.png', dpi=300, bbox_inches='tight')
print("\nâœ“ GÃ¶rsel kaydedildi: 5_ozellik_onemliligi.png")
plt.close()

# 13. DETAYLI PERFORMANS ANALÄ°ZÄ°
print("\n" + "=" * 70)
print("ğŸ”¬ DETAYLI PERFORMANS ANALÄ°ZÄ°")
print("=" * 70)

# Fiyat aralÄ±klarÄ±na gÃ¶re performans
price_ranges = [(0, 20), (20, 30), (30, 40), (40, 100)]
print("\nğŸ’° Fiyat AralÄ±klarÄ±na GÃ¶re Performans:")
print("-" * 60)

for low, high in price_ranges:
    mask = (y_test >= low) & (y_test < high)
    if mask.sum() > 0:
        range_mae = mean_absolute_error(y_test[mask], y_test_pred[mask])
        range_r2 = r2_score(y_test[mask], y_test_pred[mask])
        count = mask.sum()
        print(f"  ${low:2d}k - ${high:2d}k: MAE=${range_mae:.2f}k, RÂ²={range_r2:.3f}, N={count:3d}")

# En iyi ve en kÃ¶tÃ¼ tahminler
errors_abs = np.abs(y_test - y_test_pred)
best_indices = errors_abs.nsmallest(3).index
worst_indices = errors_abs.nlargest(3).index

print("\nâœ… En Ä°yi 3 Tahmin:")
for idx in best_indices:
    print(f"  GerÃ§ek: ${y_test.loc[idx]:.2f}k, Tahmin: ${y_test_pred[y_test.index.get_loc(idx)]:.2f}k, "
          f"Hata: ${errors_abs.loc[idx]:.2f}k")

print("\nâŒ En KÃ¶tÃ¼ 3 Tahmin:")
for idx in worst_indices:
    print(f"  GerÃ§ek: ${y_test.loc[idx]:.2f}k, Tahmin: ${y_test_pred[y_test.index.get_loc(idx)]:.2f}k, "
          f"Hata: ${errors_abs.loc[idx]:.2f}k")

# 14. Ã–ZET RAPOR
print("\n" + "=" * 70)
print("ğŸ“‹ PROJE Ã–ZET RAPORU")
print("=" * 70)

report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     BOSTON HOUSING - YAPAY SÄ°NÄ°R AÄI Ä°LE EV FÄ°YATI TAHMÄ°NÄ°      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š VERÄ° SETÄ° BÄ°LGÄ°LERÄ°:
{'â”€' * 70}
  â€¢ Toplam Ã–rnek SayÄ±sÄ±      : {len(df)}
  â€¢ Ã–zellik SayÄ±sÄ±           : {len(X.columns)}
  â€¢ EÄŸitim Seti              : {len(X_train)} Ã¶rnek (%{len(X_train)/len(df)*100:.0f})
  â€¢ Test Seti                : {len(X_test)} Ã¶rnek (%{len(X_test)/len(df)*100:.0f})
  â€¢ Veri Ã–lÃ§eklendirme       : StandardScaler

ğŸ§  MODEL MÄ°MARÄ°SÄ°:
{'â”€' * 70}
  â€¢ Model Tipi               : Multi-Layer Perceptron (MLP)
  â€¢ Gizli Katman SayÄ±sÄ±      : 4 katman
  â€¢ NÃ¶ron YapÄ±sÄ±             : [128, 64, 32, 16]
  â€¢ Aktivasyon Fonksiyonu    : ReLU (gizli), Linear (Ã§Ä±kÄ±ÅŸ)
  â€¢ Optimizer                : Adam
  â€¢ Ã–ÄŸrenme OranÄ±            : 0.001 (adaptive)
  â€¢ Batch Size               : 32
  â€¢ Max Epoch                : 1000
  â€¢ Toplam Ä°terasyon         : {model.n_iter_}
  â€¢ Early Stopping           : Aktif (patience=50)
  â€¢ L2 Regularization        : 0.001

ğŸ“ˆ PERFORMANS METRÄ°KLERÄ°:
{'â”€' * 70}

  ğŸ“ EÄÄ°TÄ°M SETÄ°:
     â”œâ”€ RÂ² Score             : {train_metrics[3]:.4f} ({train_metrics[3]*100:.2f}%)
     â”œâ”€ RMSE                 : ${train_metrics[1]:.2f}k
     â”œâ”€ MAE                  : ${train_metrics[2]:.2f}k
     â””â”€ MSE                  : {train_metrics[0]:.2f}

  ğŸ§ª TEST SETÄ°:
     â”œâ”€ RÂ² Score             : {test_metrics[3]:.4f} ({test_metrics[3]*100:.2f}%)
     â”œâ”€ RMSE                 : ${test_metrics[1]:.2f}k
     â”œâ”€ MAE                  : ${test_metrics[2]:.2f}k
     â””â”€ MSE                  : {test_metrics[0]:.2f}

  ğŸ”„ CROSS-VALIDATION (5-Fold):
     â”œâ”€ Ortalama RÂ²          : {cv_scores.mean():.4f}
     â””â”€ Standart Sapma       : {cv_scores.std():.4f}

ğŸ–ï¸ EN Ã–NEMLÄ° Ã–ZELLÄ°KLER:
{'â”€' * 70}
"""

for i, (idx, row) in enumerate(feature_importance.head(5).iterrows(), 1):
    importance_bar = "â–ˆ" * int(row['importance'] * 30)
    report += f"  {i}. {row['feature']:8} : {row['importance']:.4f} {importance_bar}\n"
    report += f"     â””â”€ {feature_descriptions[row['feature']]}\n"

report += f"""
âœ… MODEL DEÄERLENDÄ°RMESÄ°:
{'â”€' * 70}
  â€¢ Model PerformansÄ±        : {'MÃ¼kemmel' if test_metrics[3] > 0.85 else 'Ä°yi' if test_metrics[3] > 0.75 else 'Orta'}
  â€¢ GenelleÅŸtirme            : {'Ä°yi' if overfit_check < 0.05 else 'Orta' if overfit_check < 0.15 else 'ZayÄ±f'}
  â€¢ Tahmin DoÄŸruluÄŸu         : %{test_metrics[3]*100:.1f}
  â€¢ Ortalama Hata            : Â±${test_metrics[2]:.2f}k (Â±${test_metrics[2]*1000:.0f})
  
ğŸ’¡ YORUMLAR:
{'â”€' * 70}
  âœ“ Model, ev fiyatlarÄ±nÄ± yÃ¼ksek doÄŸrulukla tahmin edebiliyor
  âœ“ En etkili Ã¶zellikler: {', '.join(feature_importance.head(3)['feature'].values)}
  {'âœ“ Model iyi genelleÅŸtirilmiÅŸ, overfitting riski dÃ¼ÅŸÃ¼k' if overfit_check < 0.1 else 'âš  Hafif overfitting gÃ¶zlemlendi'}
  âœ“ Oda sayÄ±sÄ± (RM) ve dÃ¼ÅŸÃ¼k statÃ¼ oranÄ± (LSTAT) fiyatÄ± en Ã§ok etkiliyor

ğŸ“ Ã‡IKTI DOSYALARI:
{'â”€' * 70}
  â€¢ ev_fiyat_modeli.pkl      - EÄŸitilmiÅŸ yapay sinir aÄŸÄ± modeli
  â€¢ scaler.pkl               - Veri Ã¶lÃ§ekleyici (StandardScaler)
  â€¢ proje_raporu.txt         - DetaylÄ± proje raporu
  â€¢ yeni_tahmin.py           - Yeni tahmin scripti
  â€¢ 5 adet PNG gÃ¶rsel dosyasÄ± (gÃ¶rselleÅŸtirmeler)

ğŸš€ KULLANIM Ã–NERÄ°SÄ°:
{'â”€' * 70}
  # Model yÃ¼kleme:
  import pickle
  with open('ev_fiyat_modeli.pkl', 'rb') as f:
      model = pickle.load(f)
  with open('scaler.pkl', 'rb') as f:
      scaler = pickle.load(f)
  
  # Yeni tahmin:
  yeni_ev = [[0.1, 10.0, 5.0, 0, 0.5, 6.5, 70, 4.0, 3, 300, 16, 390, 10]]
  yeni_ev_scaled = scaler.transform(yeni_ev)
  prediction = model.predict(yeni_ev_scaled)
  print(f"Tahmini fiyat: ${{prediction[0]:.2f}}k")

ğŸ“š TEKNÄ°K DETAYLAR:
{'â”€' * 70}
  â€¢ KÃ¼tÃ¼phaneler: scikit-learn, pandas, numpy, matplotlib, seaborn
  â€¢ Python Versiyonu: 3.x
  â€¢ Model AlgoritmasÄ±: Backpropagation with Adam Optimizer
  â€¢ KayÄ±p Fonksiyonu: Mean Squared Error (MSE)
  â€¢ Aktivasyon: ReLU (gizli), Identity (Ã§Ä±kÄ±ÅŸ)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ‰ PROJE BAÅARIYLA TAMAMLANDI!

Yapay sinir aÄŸÄ± modeli, {len(feature_importance)} farklÄ± Ã¶zelliÄŸi kullanarak
ev fiyatlarÄ±nÄ± %{test_metrics[3]*100:.1f} doÄŸrulukla tahmin edebiliyor.

Model hazÄ±r ve kullanÄ±ma uygun! ğŸš€

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

print(report)

# Raporu kaydet
with open('/mnt/user-data/outputs/proje_raporu.txt', 'w', encoding='utf-8') as f:
    f.write(report)
print("\nâœ“ Rapor kaydedildi: proje_raporu.txt")

# 15. BONUS: YENÄ° TAHMÄ°N Ã–RNEÄÄ° SCRIPT
print("\n" + "=" * 70)
print("ğŸ BONUS: YENÄ° EV TAHMÄ°N SCRIPTI")
print("=" * 70)

prediction_script = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
Yeni Ev FiyatÄ± Tahmin Scripti
Bu script eÄŸitilmiÅŸ modeli kullanarak yeni ev Ã¶zelliklerine gÃ¶re fiyat tahmini yapar.
'''

import pickle
import numpy as np

# Model ve scaler'Ä± yÃ¼kle
print("Model yÃ¼kleniyor...")
with open('ev_fiyat_modeli.pkl', 'rb') as f:
    model = pickle.load(f)
    
with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

print("âœ“ Model baÅŸarÄ±yla yÃ¼klendi!\\n")

# Ã–zellik aÃ§Ä±klamalarÄ±
features = {
    'CRIM': 'SuÃ§ oranÄ±',
    'ZN': 'Konut alanÄ± oranÄ± (>25,000 sq.ft)',
    'INDUS': 'Ticari alan oranÄ±',
    'CHAS': 'Charles River yakÄ±nlÄ±ÄŸÄ± (0=HayÄ±r, 1=Evet)',
    'NOX': 'Azot oksit konsantrasyonu',
    'RM': 'Ortalama oda sayÄ±sÄ±',
    'AGE': 'Eski ev oranÄ± (1940 Ã¶ncesi) %',
    'DIS': 'Ä°stihdam merkezlerine uzaklÄ±k',
    'RAD': 'Otoyol eriÅŸim indeksi',
    'TAX': 'Emlak vergisi oranÄ±',
    'PTRATIO': 'Ã–ÄŸrenci-Ã¶ÄŸretmen oranÄ±',
    'B': 'Siyahi nÃ¼fus oranÄ±',
    'LSTAT': 'DÃ¼ÅŸÃ¼k statÃ¼lÃ¼ nÃ¼fus yÃ¼zdesi'
}

# Ã–rnek ev Ã¶zellikleri
print("=" * 70)
print("Ã–RNEK EV TAHMÄ°NLERÄ°")
print("=" * 70)

# Ã–rnek 1: LÃ¼ks ev
lux_house = [[0.02, 50.0, 3.0, 1, 0.4, 8.5, 20, 5.0, 2, 250, 14, 395, 2]]
print("\\nğŸ° Ã–rnek 1: LÃ¼ks Ev")
print("  â€¢ DÃ¼ÅŸÃ¼k suÃ§ oranÄ±, nehir kenarÄ±, 8.5 oda, yeni bina")

# Ã–rnek 2: Orta segment ev
mid_house = [[0.1, 20.0, 5.0, 0, 0.5, 6.5, 50, 4.0, 3, 300, 16, 390, 8]]
print("\\nğŸ  Ã–rnek 2: Orta Segment Ev")
print("  â€¢ Orta suÃ§ oranÄ±, 6.5 oda, orta yaÅŸta bina")

# Ã–rnek 3: Ekonomik ev
eco_house = [[0.3, 5.0, 10.0, 0, 0.6, 5.5, 80, 3.0, 5, 400, 18, 380, 15]]
print("\\nğŸ˜ï¸ Ã–rnek 3: Ekonomik Ev")
print("  â€¢ YÃ¼ksek suÃ§ oranÄ±, 5.5 oda, eski bina")

# Tahminler
houses = [lux_house, mid_house, eco_house]
house_names = ["LÃ¼ks Ev", "Orta Segment Ev", "Ekonomik Ev"]

print("\\n" + "=" * 70)
print("TAHMÄ°N SONUÃ‡LARI")
print("=" * 70 + "\\n")

for name, house in zip(house_names, houses):
    # Veriyi Ã¶lÃ§eklendir
    house_scaled = scaler.transform(house)
    
    # Tahmin yap
    prediction = model.predict(house_scaled)[0]
    
    print(f"ğŸ“ {name}:")
    print(f"   â””â”€ Tahmini Fiyat: ${prediction:.2f}k (${prediction*1000:.0f})")
    print()

print("=" * 70)
print("\\nğŸ’¡ Kendi eviniz iÃ§in tahmin yapmak isterseniz:")
print("   YukarÄ±daki feature deÄŸerlerini deÄŸiÅŸtirerek yeni tahminler yapabilirsiniz!")
"""

with open('/mnt/user-data/outputs/yeni_tahmin.py', 'w', encoding='utf-8') as f:
    f.write(prediction_script)
print("âœ“ Tahmin scripti kaydedildi: yeni_tahmin.py")

print("\n" + "=" * 70)
print("âœ… PROJE BAÅARIYLA TAMAMLANDI!")
print("=" * 70)
print("\nğŸ“ OluÅŸturulan Dosyalar:")
print("  1. ev_fiyat_modeli.pkl - EÄŸitilmiÅŸ yapay sinir aÄŸÄ± modeli")
print("  2. scaler.pkl - Veri Ã¶lÃ§ekleyici")
print("  3. proje_raporu.txt - DetaylÄ± rapor")
print("  4. yeni_tahmin.py - Yeni tahmin scripti")
print("  5. 1_veri_gorsellestirme.png")
print("  6. 2_korelasyon_matrisi.png")
print("  7. 3_egitim_sureci.png")
print("  8. 4_tahmin_sonuclari.png")
print("  9. 5_ozellik_onemliligi.png")

print("\nğŸ“ Proje Ã–zeti:")
print(f"  â€¢ Model %{test_metrics[3]*100:.1f} doÄŸrulukla Ã§alÄ±ÅŸÄ±yor")
print(f"  â€¢ Ortalama hata: Â±${test_metrics[2]:.2f}k")
print(f"  â€¢ {model.n_iter_} iterasyonda eÄŸitim tamamlandÄ±")
print("\nğŸš€ Model kullanÄ±ma hazÄ±r!")
